{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 652 Lab #6\n",
    "### Instructions\n",
    "- Complete all 6 questions in this assignment.\n",
    "- You may work with others, <b> but the work you submit must be your own </b>. You can differentiate your work by adding comments or changing the values you use to test your code. However, submitting some else's work as your own is an academic integrity violation and will be raised to academic affairs.\n",
    "- It is always better to attempt a problem as partial credit may be granted.\n",
    "\n",
    "\n",
    "### Submission Guide:\n",
    "- Submit your answers on BlackBoard by Thursday 2019-04-18.\n",
    "- The file must be either a .py or .ipynb file type.\n",
    "- <i><span style=\"color:red\">The name of the file you submit should be <i><b> ist652_lab6_lastname.py (.ipynb) </i></b>.</span>\n",
    "\n",
    "\n",
    "\n",
    "### Grading [ 6 total points ]\n",
    "For Each Questions (1-6), the following credit will be awarded:\n",
    "- 0.75 for printing the correct answer to the console.\n",
    "- 0.15 for approaching the problem efficiently.\n",
    "- 0.05 for properly documenting and commenting your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, download the Watson_Tweets.csv from BlackBoard. \n",
    "#### ( 1 ) Load the Watson_Tweets.csv file using pandas pd.read_csv() method and describe the data set.\n",
    "- NOTE: The file is pipe (|) seperated.\n",
    " - use the method parameter `sep` to properly parse and read the pipe-delimeted file\n",
    "\n",
    "##### [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>RT @IBMWatson: IBM's AI Fairness 360 Open Sour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweets\n",
       "count                                                 300\n",
       "unique                                                164\n",
       "top     RT @IBMWatson: IBM's AI Fairness 360 Open Sour...\n",
       "freq                                                    2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter your code here, printing relevant answers to console:\n",
    "import pandas as pd\n",
    "\n",
    "file = pd.read_csv(r\"C:\\Users\\KARTHEEK SP\\Desktop\\IST652\\watson_tweets.csv\", sep=\"|\") #reading the csv file \n",
    "\n",
    "file.describe() #tweets count = 300, unique tweets are 164, tweets freq = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ( 2 ) Tokenize the <u>dataframe</u> of tweets into a <u>list</u> of lowercase tokens using the NLTK TweetTokenizer method. Print the number of tokens.\n",
    "\n",
    "- Consider the method series.tolist() to convert the dataframe to a list.\n",
    "- NOTE: The expected object for analysis is a list of tokens, so carefully apply the tokenizer to the original list of tweets.\n",
    "\n",
    "##### [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7280"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter your code here, printing relevant answers to console:\n",
    "import nltk\n",
    "#nltk.download()\n",
    "lst = pd.Series.tolist(file)\n",
    "\n",
    "flat_list = []\n",
    "for sublist in lst:\n",
    "    for item in sublist:\n",
    "        flat_list = str(flat_list)+str(item)\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "word_tokenize = word_tokenize(flat_list)\n",
    "tokens = [w.lower() for w in word_tokenize]\n",
    "\n",
    "del(tokens[0:2]) #removing the first 2 values in tokens that are added by the for loop\n",
    "\n",
    "len(tokens) #there are 7280 tokens when tokenized by words_tokenize function\n",
    "\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a word and returns true if it consists only\n",
    "#   of non-alphabetic characters\n",
    "import re\n",
    "\n",
    "def alpha_filter(w):\n",
    "    # pattern to match a word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ( 3 ) Run the above code block which creates the alpha_filter function. Then, in your own words, describe what the regex query is filtering for.\n",
    "\n",
    "##### [1 point]\n",
    "\n",
    "#### Enter your answer here,<u>in english, not python:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#^ and $ are boundaries or anchors. ^ marks the start, while $ marks the end of a regular expression.\n",
    "# since ^ a-z is in square braces, the query filters for anything other than a-z (alphabets), which is then added with $ using + operator\n",
    "#to add numeric values in the items that need to be excluded from filtering\n",
    "# 'w' in pattern.match(w) matches any non-alphanumeric character [^a-zA-Z0-9]\n",
    "# Hence the regex qury is filtering for values that are non-alphanumeric \n",
    "# The function finally returns True if the pattern matched with non-alphanumeric character, else returns false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ( 4 ) Return a new list of tokens filtered by  the alpha_filter function and without StopWords.\n",
    "- Recall the stopwords list from NTLK which we covered in class:\n",
    "    - `nltk.corpus.stopwords.words('english')`\n",
    "\n",
    "##### [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['combat', 'bullying', 'watsomapp', 'teamed', 'ibm', 'create', 'ai', 'solution', 'helps', 'educators', 'proactively', 'identi…', 'https', '//t.co/scf4ljwosnibm', 'proud', 'named', 'fastcompany', 'world', 'changing', 'company', 'year', 'initiatives', 'like', 'call', 'fo…', 'https', '//t.co/djsph0cxbj', 'icymi', 'womenshistorymonth', 'launched', 'blog', 'series', 'highlighting', 'driving', 'force', 'behind', 'women', 'team…', 'https', '//t.co/sw5odliqeirt', 'ibmpolicy', 'today', 'ibm', 'endorses', 'new', 'eu', 'ethics', 'guidelines', 'trustworthy', 'ai']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3928"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter your code here, printing relevant answers to console:\n",
    "#Passing tokens through the alpha_filter function\n",
    "\n",
    "alpha_filtered=[] #new list for adding apha filtered items\n",
    "\n",
    "for i in tokens:                    #passes every single token through alpha_filter function and append that token to alpha_filtered \n",
    "    if alpha_filter(i):             #only if it is an alpha-numeric value\n",
    "        pass\n",
    "    else:\n",
    "        alpha_filtered.append(i)\n",
    "\n",
    "alpha_filtered\n",
    "\n",
    "\n",
    "# Filtering Stopwords\n",
    "from nltk.corpus import stopwords \n",
    "#from nltk.tokenize import word_tokenize \n",
    "  \n",
    "  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "#tokens is a word_tokenized file of flat_list  \n",
    "filtered_sentence = [w for w in alpha_filtered if not w in stop_words] #filtering by comparing the alpha_filtered in the list of stop_words\n",
    "   \n",
    "print(filtered_sentence[0:50]) \n",
    "len(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ( 5 ) Apply the frequency distribution to your list from question (4) and print the 50 most common tokens.\n",
    "- Note: recall the `nltk.FreqDist` method\n",
    "\n",
    "##### [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https:245\n",
      "ai:155\n",
      "ibm:83\n",
      "watson:76\n",
      "'s:47\n",
      "new:34\n",
      "ibmwatson:31\n",
      "data:28\n",
      "help:25\n",
      "world:20\n",
      "learn:20\n",
      "way:20\n",
      "technology:17\n",
      "bias:16\n",
      "people:16\n",
      "business:15\n",
      "to…:15\n",
      "create:14\n",
      "us:14\n",
      "assistant:14\n",
      "companies:14\n",
      "'re:14\n",
      "power:12\n",
      "ibmresearch:12\n",
      "service:12\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here, printing relevant answers to console:\n",
    "from nltk.probability import FreqDist\n",
    "fdist = nltk.FreqDist(w.lower() for w in filtered_sentence if w not in stop_words)\n",
    "#The top 50 common tokens are\n",
    "for word, frequency in fdist.most_common(25):\n",
    "    print('%s:%d' % (word, frequency))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ( 6 ) Return the top 25 bigrams by applying a bigram frequency analysis of the tokens found in ( 4 ).\n",
    "Note: Use the ntlk methods:\n",
    "- `BigramCollocationFinder.from_words()`\n",
    "- `score_ngrams()`\n",
    "\n",
    "##### [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('//t.co/0fx0ol8rgwdespite', 'growing'),\n",
       " ('approaches', 'combine'),\n",
       " ('creates', 'custom'),\n",
       " ('custom', 'highlight'),\n",
       " ('despite', 'widely'),\n",
       " ('gender', 'equality'),\n",
       " ('growing', 'sophistication'),\n",
       " ('helped', 'send'),\n",
       " ('highlight', 'reels'),\n",
       " ('kicking', 'off'),\n",
       " ('kyle_l_wigg…to', 'overcome'),\n",
       " ('mission', 'through'),\n",
       " ('peterlavers', 'shar…'),\n",
       " ('pre-eminent', 'technical'),\n",
       " ('promising', 'a…rt'),\n",
       " ('publicized', 'advocacy'),\n",
       " ('technical', 'distinction'),\n",
       " ('technique', 'cuts'),\n",
       " ('title', \"'ibm\"),\n",
       " ('unprecedented', 'growth'),\n",
       " ('while', 'enhanci…last'),\n",
       " ('widely', 'publicized'),\n",
       " ('yet', 'close'),\n",
       " ('//t.co/aov5cys1mw5', 'reasons'),\n",
       " ('//t.co/auhk50waxulexus', 'wanted')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter your code here, printing relevant answers to console:\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(tokens) #compute frequency distribution for all the bigrams in the tokens\n",
    "finder.nbest(bigram_measures.pmi, 25) #top 25 bigram_measures\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
